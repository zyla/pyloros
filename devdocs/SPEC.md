# Redlimitador Spec

A default-deny allowlist-based HTTPS filtering proxy for controlling AI agent network access.

## Purpose of this document

It is a declarative specification of what we want this product to be: features, behavior, technical choices (libraries, protocols), configuration format, and developer experience (CI/CD, tooling, workflow). It describes *what* and *why*, not *how* — implementation details like internal APIs, struct names, or macro usage belong in code and code comments, not here. Code and infrastructure should ultimately be maintained to match the requirements here. When we want to change something, we first modify the SPEC.

## Deployment Model

The intended deployment is one proxy per VM/container running an AI agent. All outbound traffic from the agent is routed through the proxy via `HTTP_PROXY`/`HTTPS_PROXY` environment variables, giving the proxy full visibility and control over the agent's network access.

## Features

### Core
- Explicit HTTP proxy mode (clients configured via `HTTP_PROXY`/`HTTPS_PROXY` env vars)
- MITM TLS interception for HTTPS traffic via CONNECT tunnels
- Plain HTTP forwarding for non-CONNECT proxy requests (e.g. `http://` URLs used by apt-get)
- Hop-by-hop header stripping per RFC 7230 for forwarded HTTP requests
- CONNECT restricted to port 443 (non-443 CONNECT requests are blocked)
- Allowlist rule engine: requests must match at least one rule to be allowed; everything else is blocked with HTTP 451
- **Default-deny for unverifiable restrictions**: when a rule requires fine-grained inspection (e.g. branch-level body inspection for git push) but the request arrives on a code path that cannot perform that inspection (e.g. plain HTTP instead of HTTPS CONNECT), the request is blocked rather than silently allowed. If we can't verify a restriction, we deny.
- TOML configuration file

### Rule Matching
- Rules specify: method, URL pattern, optional `websocket = true` flag
- `*` wildcard matches any character sequence (including across segments) in host, path, and query
- Method `*` matches any HTTP method
- Example: `https://*.github.com/api/*` matches `https://foo.github.com/api/v1/repos`

### Git Rules

Git-specific rules provide a high-level way to control git smart HTTP operations (clone, fetch, push) without requiring users to understand the underlying protocol endpoints.

A rule has **either** `method` (HTTP rule) **or** `git` (git rule), never both. Having both is a config validation error. `websocket = true` and `git` are mutually exclusive.

```toml
# Allow clone/fetch from any repo in myorg
[[rules]]
git = "fetch"
url = "https://github.com/myorg/*"

# Allow push only to a specific repo, only to feature branches
[[rules]]
git = "push"
url = "https://github.com/myorg/deploy-tools"
branches = ["feature/*", "fix/*"]

# Allow all git operations to any github.com repo
[[rules]]
git = "*"
url = "https://github.com/*"
```

#### `git` field values

| Value   | Operations allowed   |
|---------|---------------------|
| `fetch` | clone, fetch, pull  |
| `push`  | push                |
| `*`     | all                 |

The `url` is the repo base URL (what you'd pass to `git clone`).

#### Branch restriction

The optional `branches` field restricts which refs a push can target. It is only valid on `git = "push"` or `git = "*"` rules; using it with `git = "fetch"` is a config error.

- Bare patterns like `feature/*` match against `refs/heads/feature/*`.
- Patterns starting with `refs/` are matched literally (escape hatch for tags, notes, etc.).
- Omitting `branches` means any ref is allowed.
- If a push updates multiple refs and **any** ref is disallowed, the **entire push** is blocked.
- When a push is blocked by branch restrictions, the proxy returns a proper git `receive-pack` response (HTTP 200 with `report-status` and sideband error messages) instead of HTTP 451. This allows git clients to display clear per-ref errors like `! [remote rejected] main -> main (blocked by proxy policy)`.

See `DECISIONS.md` for implementation details (smart HTTP endpoint mapping, pkt-line inspection, compilation model).

#### Git-LFS support

Git-LFS uses a separate HTTP endpoint (`POST {repo}/info/lfs/objects/batch`) to negotiate large file transfers. Git rules automatically include this endpoint so that LFS operations work without additional manual rules.

- `git = "fetch"` allows LFS **download** operations (batch requests with `"operation": "download"`)
- `git = "push"` allows LFS **upload** operations (batch requests with `"operation": "upload"`)
- `git = "*"` allows both download and upload
- The proxy inspects the JSON body of LFS batch requests to verify the `operation` field matches what the rule allows
- **Branch restrictions do not apply** to LFS. LFS blobs are content-addressed; the actual ref update goes through `git-receive-pack` which is already branch-checked
- **Plain HTTP is blocked** for LFS batch requests (same default-deny principle as branch checks — body inspection requires HTTPS)
- **Transfer URLs**: LFS batch responses contain transfer URLs (often on external hosts like S3) for the actual object upload/download. These are **not** automatically allowed — users must add separate HTTP rules for the transfer hosts (e.g., `method = "GET", url = "https://github-cloud.s3.amazonaws.com/*"`)

### Protocol Support
- HTTP/1.1
- HTTP/2
- WebSocket (upgrade detection + bidirectional frame forwarding)

### Certificate Management
- User-provided or auto-generated CA certificate/key
- Per-host certificate generation with in-memory LRU cache (1000 entries, 12h TTL)
- CLI command to generate CA cert/key pair

### CLI

subcommands:

- `run --config config.toml` — start proxy
- `generate-ca --out ./certs/` — generate CA cert/key
- `validate-config --config config.toml` — validate config file

### Logging
- Configurable log level (error/warn/info/debug/trace)
- Separate control over logging of allowed and blocked requests (e.g., log only blocked to reduce noise, or only allowed for auditing)
- Error messages for failed upstream requests must include the request method and URL for diagnostics

### Proxy Authentication

The proxy can require clients to authenticate before processing any requests. This prevents unauthorized network entities from using the proxy's credential injection and URL allowlisting capabilities — critical when the proxy is reachable over a network (e.g. Docker internal networks where other containers could connect).

- Authentication uses the HTTP Basic scheme via the `Proxy-Authorization` header (RFC 7235)
- When enabled, unauthenticated or incorrectly authenticated requests receive HTTP `407 Proxy Authentication Required` with a `Proxy-Authenticate: Basic realm="redlimitador"` header
- For CONNECT tunnels, authentication is checked on the CONNECT request before the tunnel is established
- For plain HTTP proxy requests, authentication is checked on each request
- Configured via `auth_username` and `auth_password` fields in `[proxy]` — both must be present, or both absent
- `auth_password` supports `${ENV_VAR}` placeholders, resolved at startup (same mechanism as credential injection values)
- When auth is not configured, the proxy accepts all connections (backward compatible)
- Failed auth attempts are logged at warn level (client IP, username if provided) but never log the submitted password
- The `validate-config` command reports whether auth is enabled (never prints the password)

**Client configuration:** Most HTTP clients support proxy auth via embedded credentials in the proxy URL:
```
HTTP_PROXY=http://agent:secretpass@proxy:8080
HTTPS_PROXY=http://agent:secretpass@proxy:8080
```
This works with curl, git, npm, pip, and Docker — no client-side code changes needed.

### Credential Injection
The proxy can inject credentials (API keys, tokens) into outgoing requests so the agent never sees real secrets, preventing credential exfiltration.

- Credentials are configured in `[[credentials]]` sections in the config file
- Each credential has a `type` field: `"header"` (default if omitted) or `"aws-sigv4"`
- All string values support `${ENV_VAR}` placeholders resolved at startup from environment variables
- Credentials are **not** injected for plain HTTP requests (only HTTPS CONNECT tunnel)
- The `validate-config` command displays credential count, types, and URL patterns (never secret values)
- Credential secret values are never logged; only the type and match status are logged at debug level

#### Header credentials (type = "header")

Simple header injection/replacement — the original credential type.

- Each credential specifies a URL pattern, a header name, and a header value
- At request time, if a request URL matches, the proxy injects/overwrites the specified header before forwarding upstream
- If multiple credentials match the same request and set the same header, last match wins (config file order)
- Multiple credentials matching different headers on the same request all get injected

#### AWS SigV4 credentials (type = "aws-sigv4")

Re-signs requests with real AWS credentials using AWS Signature Version 4. This allows AI agents to use fake AWS credentials while the proxy transparently re-signs with real ones.

- Each credential specifies a URL pattern, `access_key_id`, `secret_access_key`, and optionally `session_token`
- At request time, if a request URL matches, the proxy:
  1. Parses the agent's existing `Authorization` header to extract the region and service from the credential scope
  2. Strips old AWS auth headers (`Authorization`, `X-Amz-Date`, `X-Amz-Content-Sha256`, `X-Amz-Security-Token`)
  3. Re-signs the request with the real credentials using SigV4
  4. Sets the new `Authorization`, `X-Amz-Date`, `X-Amz-Content-Sha256`, and optionally `X-Amz-Security-Token` headers
- The request body is fully buffered for signing (required by SigV4 which hashes the body)
- If the original request has no parseable `Authorization` header (no region/service), the credential is skipped

## Technical Decisions

- Explicit HTTP proxy (no iptables)
- MITM with CA for HTTPS inspection
- Tokio async runtime
- rustls + rcgen for TLS (pure Rust, no OpenSSL) — see `DECISIONS.md` for evaluation
- TOML config
- `*` wildcard = multi-segment match
- HTTP 451 for blocked requests
- In-memory LRU cert cache
- clap (derive) CLI
- hyper for HTTP

## Configuration Format

```toml
[proxy]
bind_address = "127.0.0.1:8080"
ca_cert = "/path/to/ca.crt"
ca_key = "/path/to/ca.key"
# Optional: require proxy authentication (both fields required if either is set)
# auth_username = "agent"
# auth_password = "${PROXY_PASSWORD}"
# Optional: override upstream port for all CONNECT forwards (testing only)
# upstream_override_port = 9443
# Optional: PEM CA cert to trust for upstream TLS (testing only)
# upstream_tls_ca = "/path/to/upstream-ca.crt"

[logging]
level = "info"
# log_requests accepts a bool (backward compat) or a table:
#   log_requests = true              # both allowed + blocked
#   log_requests = false             # neither
#   log_requests = { allowed = true, blocked = false }  # granular
log_requests = { allowed = true, blocked = true }

[[rules]]
method = "GET"
url = "https://api.example.com/health"

[[rules]]
method = "*"
url = "https://*.github.com/*"

[[rules]]
method = "GET"
url = "wss://realtime.example.com/socket"
websocket = true

# Git-specific rules
[[rules]]
git = "fetch"
url = "https://github.com/myorg/*"

[[rules]]
git = "push"
url = "https://github.com/myorg/agent-workspace"
branches = ["feature/*", "fix/*"]

# Credential injection — inject API keys/tokens into matching requests

# Header credential (type defaults to "header" when omitted)
[[credentials]]
url = "https://api.anthropic.com/*"
header = "x-api-key"
value = "${ANTHROPIC_API_KEY}"

[[credentials]]
url = "https://api.openai.com/*"
header = "authorization"
value = "Bearer ${OPENAI_API_KEY}"

# AWS SigV4 credential — re-signs requests with real AWS credentials
[[credentials]]
type = "aws-sigv4"
url = "https://*.amazonaws.com/*"
access_key_id = "${AWS_ACCESS_KEY_ID}"
secret_access_key = "${AWS_SECRET_ACCESS_KEY}"
# session_token = "${AWS_SESSION_TOKEN}"
```

## Testing

- Unit tests where it makes sense
- End-to-end integration tests covering all features: filtering rules, plain HTTP forwarding, HTTPS (MITM), HTTP/2, WebSocket
- CLI integration tests for all subcommands (`run`, `generate-ca`, `validate-config`)
- Tests run in GitHub Actions; coverage is reported
- When testing integration with external tools (git, curl, claude CLI, etc.), always verify that traffic actually went through the proxy — don't just check that the tool succeeded. Record requests at the upstream handler or check proxy logs for expected entries.
- When testing that an activity is blocked, don't only verify that the standard tool (e.g., `git push`) fails — also verify that individual protocol requests are independently blocked, since an attacker may craft requests directly, skipping discovery/negotiation steps.

See `DECISIONS.md` for implementation details (E2E test architecture, port override mechanism).

### Binary-Level Tests

Binary-level smoke tests spawn the actual `pyloros` binary and drive it with
`curl`, configured via `http_proxy`/`HTTPS_PROXY` environment variables — the same
mechanism real clients use. Tests prefer environment variables over curl CLI flags
(e.g. `--proxy`, `--cacert`) where possible to mirror real-world usage. They verify
end-to-end behavior including config parsing, CLI argument handling, and process
lifecycle.

Binary tests should enable proxy authentication to mirror realistic deployment
configurations. Proxy credentials are passed via embedded credentials in the proxy
URL (e.g. `http://user:pass@127.0.0.1:PORT`), the same way real clients configure
them.

### Live API Tests

Binary-level tests that send real requests to external APIs (e.g. `api.anthropic.com`) through the proxy, verifying the full MITM TLS pipeline against production servers. These tests require the `claude` CLI to be installed and authenticated, and are skipped when either is unavailable (e.g. in CI).

### Mutation Testing

Mutation testing with `cargo-mutants` validates test suite quality. It is run manually (not in CI) and does not need automation. The goal is to kill all viable mutants for core logic (filtering, header manipulation, protocol handling). Surviving mutants in logging/debug/cosmetic code are acceptable.

When adding new code paths with conditional logic (especially `if`, `match`, `==`/`!=`), ensure tests exercise both branches. Default-port matching, header presence checks, and error classification are common sources of surviving mutants.

### Git Smart HTTP Tests

Integration tests verify that git smart HTTP operations (clone, push) work correctly through the proxy's HTTPS MITM pipeline using git-specific config rules (`git = "fetch"`, `git = "push"`, `git = "*"`). Tests run a local git smart HTTP server (via `git http-backend` CGI), route `git clone`/`git push` commands through the proxy, and verify end-to-end correctness.

Test coverage includes:
- Basic clone/push through proxy with git rules (`git_smart_http_test.rs`)
- Operation-level filtering: fetch-only rule blocks push, push-only blocks clone (`git_rules_test.rs`)
- Repo-level filtering: URL patterns restrict which repos are accessible (`git_rules_test.rs`)
- Branch-level restriction: `branches` patterns allow/block pushes to specific refs (`git_rules_test.rs`)
- Pkt-line parser unit tests: ref extraction, capabilities handling, branch matching (`pktline.rs`)
- Git-LFS: LFS batch endpoint filtering by operation type, plain HTTP blocking, merged-scan for combined fetch+push rules (`git_lfs_test.rs`)
- Proxy authentication: correct credentials accepted, wrong/missing credentials get 407, auth disabled works without credentials (`proxy_auth_test.rs`)

### Test Report Generation

Tests produce a human-readable report showing, for each test: what was done, what the result was, and what assertions were checked. The report is tightly coupled to actual test execution — descriptions are derived from real parameters (URLs, rules, CLI args), making drift between tests and report impossible.

- A standalone report generator tool (`tools/test-report/`) runs the test suite and produces Markdown + HTML output.
- The Markdown report is published to the GitHub Actions job summary so it's visible directly in the run without downloading artifacts.
- Reports are also uploaded as CI artifacts.

Test actions (HTTP requests, CLI invocations, etc.) should be performed through wrapper functions that both execute the action and emit a matching report entry. Bare `t.action()` + manual code pairs are not acceptable — the action description and execution must be coupled in a single API call so they can't drift apart. Examples: `ReportingClient` for HTTP requests, `_reported()` variants of test helpers.

### Fuzzing

Fuzz testing with `cargo-fuzz` (libFuzzer) targets parser and matching code that handles untrusted input. Targets: pkt-line parsing, pattern matching, URL pattern parsing, config parsing. Run manually, not in CI. Seed corpora live in `fuzz/seeds/<target>/`.

## Distribution

Statically-linked Linux x86_64 binaries (musl) are published as GitHub Release assets on version tags (`v*`). The release workflow builds the binary, runs tests against it, verifies static linking, and packages it as a tarball with SHA256 checksums.

A rolling `latest` pre-release is built from `main` on every push. It uses a fixed `latest` git tag (force-moved to HEAD) and is marked as a prerelease with `make_latest: false` so it doesn't override the versioned "Latest release" in the GitHub UI.

### Docker Image

A Docker image is published to `ghcr.io/zyla/pyloros` with the following tags:
- `v1.2.3` + `latest` — on version tags
- `edge` — on every push to `main`

The image uses Alpine as the base (~7MB overhead), containing only the statically-linked binary. The same release workflow that publishes GitHub Release assets also builds and pushes the Docker image.

The Docker Compose example and sandbox script default to the published image, so users can start immediately without building from source.

### Docker Sandbox

A helper script (`scripts/docker-sandbox.sh`) runs a Docker container with all network access
routed exclusively through the pyloros proxy using Docker internal networks. The sandbox
container is placed on an `--internal` Docker network with no direct internet access; the proxy
container bridges the internal and external networks, forwarding only allowed requests.

When proxy authentication is enabled, the sandbox script passes the proxy URL with embedded
credentials to the workload container via environment variables.

### Docker Compose Example

A Docker Compose example (`examples/docker-compose/`) provides a declarative alternative to the
imperative sandbox script, with the same two-network architecture (external bridge + internal
isolated). A test script (`scripts/test-docker-compose.sh`) verifies allowed/blocked behavior and
network isolation.

When proxy authentication is enabled, the compose file passes the proxy secret to the workload
container via Docker Compose environment variables or secrets.

## Documentation

The project README (`README.md`) must contain:

- Project name, tagline, and brief description of what it does and why
- Overview of the deployment model (one proxy per VM/container)
- Quick-start guide: generate CA, create config, start proxy, configure client
- Per-tool client configuration guide (curl, git, Node.js/Claude Code) covering proxy env vars, CA cert setup, and tool-specific gotchas
- Configuration reference with example covering `[proxy]`, `[logging]`, and `[[rules]]` sections
- CLI reference for all subcommands (`run`, `generate-ca`, `validate-config`) with flags
- Build from source instructions (prerequisites, cargo build)
- How to run tests
- License (MIT)
